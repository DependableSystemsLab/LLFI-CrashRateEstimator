# LLFI Crash Rate Estimator

LLFI Crash Rate Estimator is a framework for generating a estimate of a program's crash probability that matches more closely to that as measured using assembly-level fault injection (i.e., [PINFI](https://github.com/DependableSystemsLab/PINFI)). This machine learning-based technique uses the crash probability as measured using [LLFI](https://github.com/DependableSystemsLab/LLFI) along with the program's percentage of instructions that operate on memory address values as inputs to calculate the estimation.

## Overview

Research has shows that [IR-level fault injection](https://github.com/DependableSystemsLab/LLFI) is as accurate as [assembly-level tools](https://github.com/DependableSystemsLab/PINFI) for measuring errors that result in silent data corruptions (SDCs) \[[1](https://doi.org/10.1109/DSN.2014.2), [2](http://blogs.ubc.ca/karthik/files/2019/08/issre19-paper.pdf)\]. However, measuring a program's crash probability using IR-level FI still proves to be inaccurate when compared with assembly-level FI \[[2](http://blogs.ubc.ca/karthik/files/2019/08/issre19-paper.pdf)\]. This tool uses machine learning to provide more accurate measurements of a program's crash probability.

For a detailed description of the technique, refer to [Chapters 6-7 here](http://hdl.handle.net/2429/71948).

## Setup

To use this tool, the following must be installed:

1. [Intel PIN v3.5](https://github.com/DependableSystemsLab/ISSRE19/tree/master/Tools/PIN-v3.5)
2. Our custom PIN tool found in the ./memaddr-pintool/ folder. Copy the folder and its contents to the source/tools/ directory of the PIN installation and run "make" in the folder to install. Make note of the install path.
3. [Python3](https://www.python.org/download/releases/3.0/) to run the scripts
4. The following Python libraries (these can be installed all at once by installing [Anaconda](https://docs.continuum.io/anaconda/)) :
    - [NumPy](https://numpy.org/)
    - [SciPy](https://scipy.org/install.html)
    - [scikit-learn](https://scikit-learn.org/stable/install.html)
    - [pandas](https://pandas.pydata.org/pandas-docs/stable/install.html)

You must also have [LLFI](https://github.com/DependableSystemsLab/LLFI) and [PINFI](https://github.com/DependableSystemsLab/PINFI) installed for running your own respective FI experiments.
    
## General Instructions

NOTE: We strongly recommend training and choosing a model that performs best with your own training dataset, i.e., one that reflects the types of programs you wish to make estimations for. This research tool is meant to be a framework for generating a prediction model to be used in conjunction with fault injection studies, and the dataset used to train the model that is ultimately used should be an accurate representation of the programs under test.

We do provide a sample dataset of 100 data points, collected from our own research studies, along with a saved model that has been pre-trained on the provided sample data. The generalizability of this specific model has not been thoroughly evaluated and using this model out-of-the-box for uses other than general research purposes is not recommended.

### Step 1: Profile code for memory address instructions

Scripts to profile the compiled IR and assembly code are found in the directory ./memaddr-scripts/.

To invoke, run main.py in its root folder using python3. Each 
program you wish to measure should have its own respective subfolder 
within the ./memaddr-scripts/benchmarks/ folder, with each folder containing the following (samples are provided in the directory ./memaddr-scripts/benchmarks-sample/):

 - a text file with any command line input (filename: input.txt)
 - any other input files required for that program
 - the x86 executable (filename: x86) 
 - the LLFI-indexed LLVM IR (filename: ir.ll)
       -this should be the .ll file generated by LLFI during the profiling pass, found in the llfi experiment directory under "llfi/filename-llfi_index.ll"
 - the fault injection logs as generated by LLFI 
   (filename: llfi.stat.fi.injectedfaults.txt)
       -if this file was not generated by LLFI, place the llfi\_stat\_output folder in this directory instead (this script will use these logs instead of the llfi.stat.fi.injectedfaults.txt file but it is slightly more time-consuming)

You are also required to have a specifc PIN tool installed, see the above setup requirements.

NOTE: make sure the hardcoded filepaths for PIN are correct to your setup, see globals.py file

The results will be saved to a .csv file. The dynamic and static instruction percentages are both saved, however only the dynamic percentages are used to make predictions using the model.

### Step 2: Organize your input data

The input data should be stored in a .csv file; a sample .csv file with data in the expected format ia provided (./estimator/sample-data.csv). Each data point is a compiled program that has accompanying fault injection data, the features (columns) are described below:

- "mem-addr IR": this feature is measured in Step 1 as "IR dyn percent", refer to the generated .csv file for these values
- "mem-addr x86": this feature is measured in Step 1 as "x86 dyn percent", refer to the generated .csv file for these values
- "Crash LLFI": the crash probability as measured using LLFI, record these values from your own LLFI fault injection experiments 
- "Crash PINFI": the crash probability as measured using PINFI, record these values from your own PINFI fault injection experiments 

All values should be percentages formatted as decimals.

### Step 3: Train a model on your data

To train a variety of different machine learning models, run the python script ./estimator/train_models.py. 

This script will save each trained model as a .joblib file, which can then be used to make estimations on new data in the next step. The performances of each model (i.e., validation error, test error, R2, etc) can be found in a generated .csv file. We recommend choosing a model based on lowest test mean-squared-error (MSE). 

### Step 4: Make estimations using a saved model

Run the script ./estimator/predict.py to make estimations on new data. Pass as arguments the following:

- input data spreadsheet: a spreadsheet containing the input data you wish to make estimations on, formatted like the provided ./estimator/sample-predict.csv
- the saved model .joblib file

Example usage: `python3 predict.py input_data.csv model.joblib`

The crash rate estimates for each program in your input file will be stored in a new .csv file.
